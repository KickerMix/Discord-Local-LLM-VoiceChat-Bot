import os
import json
import asyncio
import logging
import re
from pathlib import Path
import interactions
from interactions import (
    Client,
    slash_command,
    SlashContext,
    SlashCommandOption,
    SlashCommandChoice,
    OptionType,
    Intents,
    Button,
    ButtonStyle,
    ComponentContext,
    Embed,
)
from interactions.api.events import Startup
from interactions.api.voice.audio import AudioVolume
from faster_whisper import WhisperModel
from TTS.api import TTS
from dotenv import load_dotenv
import aiohttp
from datetime import datetime

# ‚îÄ‚îÄ –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
logging.basicConfig(
    level=logging.DEBUG,
    format="[%(asctime)s] %(levelname)s:%(name)s: %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler("bot_debug.log", encoding="utf-8")
    ]
)
logger = logging.getLogger(__name__)

# ‚îÄ‚îÄ –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –Ω–∞—Å—Ç—Ä–æ–µ–∫ –∏ –¥–µ—Ñ–æ–ª—Ç—ã ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
SETTINGS_PATH = Path("settings.json")
DEFAULT_SETTINGS = {
    "TTS_MODE": "default",
    "selected_file_path": "./sample/sample_miside.wav"
}

def load_settings():
    if SETTINGS_PATH.exists():
        try:
            return json.loads(SETTINGS_PATH.read_text(encoding="utf-8"))
        except json.JSONDecodeError:
            logger.warning("settings.json –ø–æ–≤—Ä–µ–∂–¥—ë–Ω, –ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏")
    save_settings(DEFAULT_SETTINGS)
    return DEFAULT_SETTINGS.copy()

def save_settings(settings: dict):
    SETTINGS_PATH.write_text(json.dumps(settings, ensure_ascii=False, indent=2), encoding="utf-8")

# ‚îÄ‚îÄ –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–∫ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
settings = load_settings()
TTS_MODE = settings["TTS_MODE"]
current_speaker_wav = settings["selected_file_path"]

# ‚îÄ‚îÄ –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏ –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
load_dotenv()
TOKEN = os.getenv("DISCORD_TOKEN")
LM_STUDIO_API_URL = os.getenv("LM_STUDIO_API_URL", "http://127.0.0.1:5000")
ROLE = os.getenv("ROLE", "")
KEYWORDS = [w.strip() for w in os.getenv("KEYWORDS", "").split(",") if w.strip()]

AUDIO_DIR = Path(os.path.abspath("./audio"))
AUDIO_DIR.mkdir(parents=True, exist_ok=True)
GUILD_ID = 188965959707525120  # –∑–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ –≤–∞—à guild id

# –ö–æ–º–ø–∏–ª—è—Ü–∏—è regex –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
pattern = (r"\b(?:" + "|".join(re.escape(w) for w in KEYWORDS) + r")\b") if KEYWORDS else None
KEYWORD_RE = re.compile(pattern, flags=re.IGNORECASE) if pattern else None

# ‚îÄ‚îÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Whisper –∏ TTS ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
stt_model = WhisperModel(
    model_size_or_path="large-v2",
    device="cuda",
    compute_type="int8"
)
tts = TTS(model_name="multilingual/multi-dataset/xtts_v2")
tts.to("cuda")

# ‚îÄ‚îÄ –û—Å–Ω–æ–≤–Ω–æ–π –±–æ—Ç —Å –≥–æ–ª–æ—Å–æ–≤–æ–π –∑–∞–ø–∏—Å—å—é ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
intents = Intents.ALL
bot = interactions.Client(token=TOKEN, intents=intents)

recording_lock = asyncio.Lock()
is_connected = False
is_playing_response = False
current_channel = None

@bot.listen(Startup)
async def on_startup():
    global is_connected  # –¥–æ–±–∞–≤–ª—è–µ–º –¥–ª—è —è–≤–Ω–æ–≥–æ –∏–∑–º–µ–Ω–µ–Ω–∏—è –≥–ª–æ–±–∞–ª—å–Ω–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
    is_connected = False  # —Å–±—Ä–∞—Å—ã–≤–∞–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ

    guild = await bot.fetch_guild(GUILD_ID)
    all_states = guild.voice_states

    # –ò—â–µ–º –∫–∞–Ω–∞–ª —Å –æ–¥–Ω–∏–º —á–µ–ª–æ–≤–µ–∫–æ–º
    for channel in guild.channels:
        if channel.type is not interactions.ChannelType.GUILD_VOICE:
            continue
        members_states = [vs for vs in all_states if vs.channel and vs.channel.id == channel.id]
        if len(members_states) == 1:  # –ö–∞–Ω–∞–ª —Å –æ–¥–Ω–∏–º —á–µ–ª–æ–≤–µ–∫–æ–º
            vs_state = members_states[0]
            member = vs_state.member
            nick = member.display_name
            voice = await channel.connect()
            logger.info(f"–ê–≤—Ç–æ–ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ {nick} –≤ –∫–∞–Ω–∞–ª–µ {channel.name}")
            prompt = f'–ü–æ–ø—Ä–∏–≤–µ—Ç—Å—Ç–≤—É–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è "{nick}" –∏ –ø—Ä–µ–¥—Å—Ç–∞–≤—å—Å—è —Å–≤–æ–∏–º –∏–º–µ–Ω–µ–º "–°–∞—è".'
            async with aiohttp.ClientSession() as sess:
                payload = {
                    "model": "your-model-id",
                    "messages": [{"role": "system", "content": ROLE}, {"role": "user", "content": prompt}],
                    "max_tokens": 350
                }
                resp = await sess.post(f"{LM_STUDIO_API_URL}/v1/chat/completions", json=payload)
                data = await resp.json()
            greeting = data["choices"][0]["message"]["content"]
            ts = datetime.now().strftime("%Y%m%d%H%M%S")
            out_path = AUDIO_DIR / f"greet_{ts}.wav"
            await asyncio.to_thread(
                tts.tts_to_file,
                text=greeting,
                speaker="Ana Florence",
                language="ru",
                file_path=str(out_path)
            )
            await voice.play(AudioVolume(str(out_path)))
            await asyncio.sleep(2)
            await voice.play(AudioVolume("join.wav"))
            asyncio.create_task(start_recording(voice, str(AUDIO_DIR)))
            break
    else:
        # –ï—Å–ª–∏ –∫–∞–Ω–∞–ª —Å –æ–¥–Ω–∏–º —á–µ–ª–æ–≤–µ–∫–æ–º –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—â–µ–º –∫–∞–Ω–∞–ª —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –ª—é–¥—å–º–∏
        for channel in guild.channels:
            if channel.type is not interactions.ChannelType.GUILD_VOICE:
                continue
            members_states = [vs for vs in all_states if vs.channel and vs.channel.id == channel.id]
            if len(members_states) > 1:  # –ö–∞–Ω–∞–ª —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –ª—é–¥—å–º–∏
                voice = await channel.connect()
                logger.info(f"–ê–≤—Ç–æ–ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –∫–∞–Ω–∞–ª—É {channel.name} —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –ª—é–¥—å–º–∏.")
                await asyncio.sleep(2)  # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ, –∑–∞—Ç–µ–º –Ω–∞—á–∏–Ω–∞–µ–º –ø—Ä–æ—Å–ª—É—à–∏–≤–∞—Ç—å
                await voice.play(AudioVolume("join.wav"))
                asyncio.create_task(start_recording(voice, str(AUDIO_DIR)))
                break

    logger.info("–ë–æ—Ç –ø–µ—Ä–µ—à—ë–ª –≤ —Ä–µ–∂–∏–º –æ–∂–∏–¥–∞–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤.")

async def reconnect_voice(channel):
    try:
        if channel:
            return await channel.connect()
        logger.error("–ù–µ –Ω–∞–π–¥–µ–Ω –∫–∞–Ω–∞–ª –¥–ª—è —Ä–µ–∫–æ–Ω–Ω–µ–∫—Ç–∞.")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–µ–∫–æ–Ω–Ω–µ–∫—Ç–µ –≤ –≥–æ–ª–æ—Å–æ–≤–æ–π –∫–∞–Ω–∞–ª: {e}")
    return None

async def start_recording(voice_state, output_dir: str):
    global current_channel
    if not voice_state:
        voice_state = await reconnect_voice(current_channel) if current_channel else None
        if not voice_state:
            logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ.")
            return
    async with recording_lock:
        await voice_state.start_recording(output_dir=output_dir, encoding="wav")
    await asyncio.sleep(3)
    await stop_recording(voice_state)

    for user_id, file in list(voice_state.recorder.output.items()):
        audio_path = file.get("path") if isinstance(file, dict) else file
        if os.path.exists(audio_path):
            await process_audio_for_user(voice_state, user_id, audio_path)

    if not is_playing_response:
        await start_recording(voice_state, output_dir)

async def stop_recording(voice_state):
    async with recording_lock:
        await voice_state.stop_recording()

async def transcribe_audio(audio_path: str) -> str:
    def _transcribe_blocking(path):
        segments, info = stt_model.transcribe(path, language="ru", beam_size=5)
        return segments, info

    try:
        segments, info = await asyncio.to_thread(_transcribe_blocking, audio_path)
        texts = [seg.text for seg in segments if hasattr(seg, 'text')]
        logger.debug(f"Detected language '{info.language}' (p={info.language_probability:.2f})")
        return " ".join(texts)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏: {e}")
        return ""

async def process_audio_for_user(voice_state, user_id: int, audio_path: str):
    global is_playing_response
    full_text = await transcribe_audio(audio_path)
    logger.debug(f"–¢–µ–∫—Å—Ç: {full_text}")
    if KEYWORD_RE and KEYWORD_RE.search(full_text):
        is_playing_response = True
        await voice_state.play(AudioVolume("trigger.wav"))
        await process_audio_for_individual_user(
            voice_state,
            user_id,
            full_text,
            speaker_wav=current_speaker_wav
        )

async def process_audio_for_individual_user(voice_state, user_id, initial_text, speaker_wav: str):
    global is_playing_response, current_channel
    # –û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –∑–∞–ø–∏—Å–µ–π
    for old in AUDIO_DIR.rglob("*.wav"):
        old.unlink()
    voice_state.recorder.output.clear()

    now = datetime.now().strftime("%Y%m%d%H%M%S")
    user_dir = AUDIO_DIR / f"individual_{user_id}_{now}"
    user_dir.mkdir(parents=True, exist_ok=True)

    await voice_state.start_recording(output_dir=str(user_dir), encoding="wav")
    await asyncio.sleep(10)
    await stop_recording(voice_state)

    if not voice_state:
        voice_state = await reconnect_voice(current_channel) if current_channel else None
        if not voice_state:
            logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ.")
            return

    await voice_state.play(AudioVolume("listening.wav"))
    data = voice_state.recorder.output.get(user_id)
    audio_path = data.get("path") if isinstance(data, dict) else data
    if not audio_path or not os.path.exists(audio_path):
        logger.error(f"–ê—É–¥–∏–æ—Ñ–∞–π–ª –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {audio_path}")
        return

    followup = await transcribe_audio(audio_path)
    full_text = f"{initial_text} {followup}".strip()
    logger.debug(f"–¢–µ–∫—Å—Ç: {followup}")

    if TTS_MODE == "default":
        await generate_and_play_response(voice_state, user_id, full_text)
    else:
        await generate_and_play_voiceclone_response(
            voice_state, user_id, full_text, speaker_wav
        )

async def generate_and_play_response(voice_state, user_id, text: str):
    """
    Pipeline TTS responses one sentence at a time to avoid GPU overload.
    """
    global is_playing_response
    # 1) Get LLM answer
    ts = datetime.now().strftime("%Y%m%d%H%M%S")
    async with aiohttp.ClientSession() as session:
        payload = {
            "model": "your-model-id",
            "messages": [{"role": "system", "content": ROLE}, {"role": "user", "content": text}],
            "max_tokens": 1000
        }
        resp = await session.post(f"{LM_STUDIO_API_URL}/v1/chat/completions", json=payload)
        data = await resp.json()
        answer = data["choices"][0]["message"]["content"]

    # 2) Split into sentences
    sentences = re.split(r'(?<=[\.!\?])\s+', answer.strip())
    if not sentences:
        return

    # Helper: run TTS for one sentence
    def tts_to_file(sentence, idx):
        out = AUDIO_DIR / f"response_{ts}_{idx}.wav"
        tts.tts_to_file(
            text=sentence,
            speaker="Ana Florence",
            language="ru",
            file_path=str(out)
        )
        return str(out)

    is_playing_response = True

    # 3) Generate and play first sentence synchronously
    first_wav = await asyncio.to_thread(tts_to_file, sentences[0], 0)
    play_task = asyncio.create_task(voice_state.play(AudioVolume(first_wav)))

    # 4) Now pipeline each remaining sentence one by one
    for idx, sent in enumerate(sentences[1:], start=1):
        # While the previous is still playing, generate this one on the side
        next_wav = await asyncio.to_thread(tts_to_file, sent, idx)

        # Wait for the previous playback to finish
        await play_task

        # Play this sentence, then loop to generate+queue the next
        play_task = asyncio.create_task(voice_state.play(AudioVolume(next_wav)))

    # 5) Wait for the final sentence to finish
    await play_task
    is_playing_response = False

async def generate_and_play_voiceclone_response(voice_state, user_id, text: str, speaker_wav: str):
    """
    Pipeline TTS-clone responses one at a time to avoid GPU overload.
    """
    global is_playing_response
    # 1) Get LLM answer
    ts = datetime.now().strftime("%Y%m%d%H%M%S")
    async with aiohttp.ClientSession() as session:
        payload = {
            "model": "your-model-id",
            "messages": [{"role": "system", "content": ROLE}, {"role": "user", "content": text}],
            "max_tokens": 1000
        }
        resp = await session.post(f"{LM_STUDIO_API_URL}/v1/chat/completions", json=payload)
        data = await resp.json()
        answer = data["choices"][0]["message"]["content"]

    # 2) Split into sentences
    sentences = re.split(r'(?<=[\.!\?])\s+', answer.strip())
    if not sentences:
        return

    # Helper: run TTS-clone for one sentence
    def tts_clone_to_file(sentence, idx):
        out = AUDIO_DIR / f"clone_{ts}_{idx}.wav"
        tts.tts_to_file(
            text=sentence,
            speaker_wav=speaker_wav,
            language="ru",
            file_path=str(out)
        )
        return str(out)

    is_playing_response = True

    # 3) Generate and play first sentence synchronously
    first_wav = await asyncio.to_thread(tts_clone_to_file, sentences[0], 0)
    play_task = asyncio.create_task(voice_state.play(AudioVolume(first_wav)))

    # 4) Now pipeline each remaining sentence one by one
    for idx, sent in enumerate(sentences[1:], start=1):
        # While the previous is still playing, generate this one on the side
        next_wav = await asyncio.to_thread(tts_clone_to_file, sent, idx)

        # Wait for the previous playback to finish
        await play_task

        # Play this sentence, then loop to generate+queue the next
        play_task = asyncio.create_task(voice_state.play(AudioVolume(next_wav)))

    # 5) Wait for the final sentence to finish
    await play_task
    is_playing_response = False

# ‚îÄ‚îÄ –°–ª—ç—à-–∫–æ–º–∞–Ω–¥—ã ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
@slash_command(
    name="join",
    description="–ü–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ –≥–æ–ª–æ—Å–æ–≤–æ–º—É –∫–∞–Ω–∞–ª—É."
)
async def join(ctx: SlashContext):
    global is_connected, current_channel
    if is_connected:
        return await ctx.send("–Ø —É–∂–µ –ø–æ–¥–∫–ª—é—á—ë–Ω.")
    if not ctx.author.voice:
        return await ctx.send("‚ùó –í–æ–π–¥–∏—Ç–µ –≤ –≥–æ–ª–æ—Å–æ–≤–æ–π –∫–∞–Ω–∞–ª –∏ –ø–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞.")
    vs = await ctx.author.voice.channel.connect()
    is_connected = True
    current_channel = ctx.author.voice.channel
    await ctx.send("‚úÖ –ü–æ–¥–∫–ª—é—á–∏–ª—Å—è.")
    asyncio.create_task(start_recording(vs, str(AUDIO_DIR)))

@slash_command(
    name="leave",
    description="–û—Ç–∫–ª—é—á–∏—Ç—å—Å—è –æ—Ç –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ –∫–∞–Ω–∞–ª–∞."
)
async def leave(ctx: SlashContext):
    global is_connected, current_channel
    if ctx.voice_state:
        await ctx.voice_state.disconnect()
        is_connected = False
        current_channel = None
        await ctx.send("üõë –û—Ç–∫–ª—é—á–∏–ª—Å—è.")
    else:
        await ctx.send("ü§∑ –Ø –Ω–µ –ø–æ–¥–∫–ª—é—á—ë–Ω.")

@slash_command(
    name="saya_tts",
    description="–ü–µ—Ä–µ–∫–ª—é—á–∏—Ç—å —Ä–µ–∂–∏–º TTS: default –∏–ª–∏ clone",
    options=[
        SlashCommandOption(
            name="mode",
            description="–í—ã–±–µ—Ä–∏—Ç–µ —Ä–µ–∂–∏–º TTS",
            required=True,
            type=OptionType.STRING,
            choices=[
                SlashCommandChoice(name="default", value="default"),
                SlashCommandChoice(name="clone",   value="clone"),
            ],
        )
    ]
)
async def saya_tts(ctx: SlashContext, mode: str):
    global TTS_MODE, settings
    if mode not in ("default", "clone"):
        return await ctx.send("‚ùó –ù–µ–≤–µ—Ä–Ω—ã–π —Ä–µ–∂–∏–º, –≤—ã–±–µ—Ä–∏—Ç–µ default –∏–ª–∏ clone.", ephemeral=True)
    TTS_MODE = mode
    settings["TTS_MODE"] = TTS_MODE
    save_settings(settings)
    await ctx.send(f"‚úÖ –†–µ–∂–∏–º TTS —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω: `{mode}`")

# ‚îÄ‚îÄ –ü–∞–≥–∏–Ω–∞—Ü–∏—è –¥–ª—è —Å–ø–∏—Å–∫–∞ —Ñ–∞–π–ª–æ–≤ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def generate_file_list_page(files, page=1, items_per_page=5):
    start_idx = (page - 1) * items_per_page
    end_idx = page * items_per_page
    selected_files = files[start_idx:end_idx]

    embed = Embed(
        title="–°–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö sample-—Ñ–∞–π–ª–æ–≤",
        description="–ù–∞–∂–º–∏—Ç–µ –Ω–∞ –∫–Ω–æ–ø–∫—É —Å –Ω–æ–º–µ—Ä–æ–º —Ñ–∞–π–ª–∞, —á—Ç–æ–±—ã –≤—ã–±—Ä–∞—Ç—å."
    )
    for idx, fname in enumerate(selected_files, start=start_idx + 1):
        embed.add_field(name=f"{idx}. {fname}", value=fname, inline=False)

    components = []
    if page > 1:
        components.append(Button(label="¬´ –ù–∞–∑–∞–¥", custom_id="prev_page", style=ButtonStyle.SECONDARY))
    if end_idx < len(files):
        components.append(Button(label="–í–ø–µ—Ä—ë–¥ ¬ª", custom_id="next_page", style=ButtonStyle.SECONDARY))

    for idx, fname in enumerate(selected_files, start=start_idx + 1):
        components.append(Button(
            label=str(idx),
            custom_id=f"select_{fname}",
            style=ButtonStyle.PRIMARY
        ))

    return embed, components

@slash_command(name="saya_list", description="–ü–æ–∫–∞–∑–∞—Ç—å —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –∏ –≤—ã–±—Ä–∞—Ç—å –æ–¥–∏–Ω.")
async def saya_list(ctx: SlashContext):
    global current_speaker_wav, settings

    sample_files = [f.name for f in Path("./sample").glob("*.wav")]
    if not sample_files:
        return await ctx.send("‚ùó –ù–µ—Ç `.wav` –≤ –ø–∞–ø–∫–µ `./sample`.")

    page = 1
    embed, components = generate_file_list_page(sample_files, page)
    await ctx.send(embed=embed, components=components, ephemeral=True)

    while True:
        used = await bot.wait_for_component(
            components=components,
            check=lambda c: c.ctx.author.id == ctx.author.id,
            timeout=60
        )

        cid = used.ctx.custom_id
        if cid == "prev_page":
            page -= 1
        elif cid == "next_page":
            page += 1
        elif cid.startswith("select_"):
            fname = cid.split("_", 1)[1]
            current_speaker_wav = f"./sample/{fname}"
            settings["selected_file_path"] = current_speaker_wav
            save_settings(settings)

            # —Ä–µ–¥–∞–∫—Ç–∏—Ä—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª –∏ —É–±–∏—Ä–∞–µ–º –∫–Ω–æ–ø–∫–∏
            await used.ctx.edit_origin(
                embed=Embed(
                    title="–ì–æ—Ç–æ–≤–æ ! üéâ",
                    description=f"–í—ã–±—Ä–∞–Ω —Ñ–∞–π–ª –¥–ª—è –∫–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è: `{fname}`"
                ),
                components=[]
            )
            # –∏, –µ—Å–ª–∏ —Ö–æ—Ç–∏—Ç–µ, —à–ª—ë–º –æ—Ç–¥–µ–ª—å–Ω–æ–µ —ç–øhemeral-–ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ
            await used.ctx.send("–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ ‚úÖ", ephemeral=True)
            return

        embed, components = generate_file_list_page(sample_files, page)
        await used.ctx.edit_origin(embed=embed, components=components)

bot.start()
